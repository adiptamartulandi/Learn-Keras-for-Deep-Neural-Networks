{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- Created by: Adipta Martulandi\n",
    "- Email : adipta.martulandi@gmail.com\n",
    "- LinkedIn : https://www.linkedin.com/in/adiptamartulandi/\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Setting Up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To follow this chapter, you must have several packages and python installed in your machine:\n",
    "    1. Python 3.x\n",
    "    2. Keras\n",
    "    3. Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Getting Started with DL in Keras\n",
    "- Let’s start by studying the DNN and its logical components, understanding what each component is used for and how these building blocks are mapped in the Keras framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 First Component 'Input Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input data for a DL algorithm can be of a variety of types. Essentially, the model understands data as “tensors”. Tensors are nothing but a generic form for vectors, or in computer engineering terms, a simple n-dimensional matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Additionally, DL models can interpret only numeric data. If the dataset has any categorical data like “gender” with values of “male” and “female,” we will need to convert them to one-hot encoded variables (0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Image data also needs to be transformed into an n-dimensional tensor. An image is stored in data as a three-dimensional tensor where two dimensions define the pixel values on a 2D plane and a third dimension defines the values for RGB color channels.\n",
    "- So essentially, one image becomes a three-dimensional tensor and n images will be a four-dimensional tensor, where the fourth dimension will stack a 3D tensor image as a training sample. Therefore, if we have 100 images with a 512 × 512-pixel resolution, they will be represented as a 4D tensor with shape 512 × 512 × 3 × 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lastly, it is a good practice to normalize, standardize, or scale the input values before training. Normalizing the values will bring all values in the input tensor into a 0–1 range, and standardization will bring the values into a range where the mean is 0 and the standard deviation is 1.\n",
    "- This helps to reduce computation, as the learning improves by a great margin and so does performance, as the activation functions (covered in the following) behave more appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the core of the DNN, we have neurons where computation for an output is executed. A neuron receives one or more inputs from the neurons in the previous layer. If the neurons are in the first hidden layer, they will receive the data from the input data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An activation function is the function that takes the combined input values, applies a function on it, and passes the output value, thus trying to mimic the activate/deactivate function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To keep things simple, we would always need a nonlinear activation function (at least in all hidden layers) to get the network to learn properly.\n",
    "- There are a variety of choices available to use as an activation function. The most common ones are the sigmoid function and the ReLU (rectified linear unit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.1 Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A sigmoid function is defined as which renders the output between 0 and 1.\n",
    "- The nonlinear\n",
    "output (s shaped as shown) improves the learning process very well, as it\n",
    "3closely resembles the following principle—lower influence: low output and\n",
    "higher influence: higher output—and also confines the output within the\n",
    "0-to-1 range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ReLU uses the function f(z) = max(0,z), which means that if the output is positive it would output the same value, otherwise it would output 0. The function’s output range is shown in the following visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The overall structure of a DNN is developed using the model object in Keras. This provides a simple way to create a stack of layers by adding new layers one after the other.\n",
    "- The easiest way to define a model is by using the sequential model, which allows easy creation of a linear stack of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A layer in the DNN is defined as a group of neurons or a logically separated group in a hierarchical network structure. As DL became more and more popular, there were several experiments conducted with network architectures to improve performance for a variety of use cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras provides us with several types of layers and various means to connect them. We will take a close look at a few layers and also glance through some important layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 Core Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6.1 Dense Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A dense layer is a regular DNN layer that connects every neuron in the defined layer to every neuron in the previous layer. For instance, if Layer 1 has 5 neurons and Layer 2 (dense layer) has 3 neurons, the total number of connections between Layer 1 and Layer 2 would be 15 (5 × 3).\n",
    "- We also need to define the input shape for the Keras layer. The input shape needs to be defined for only the first layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6.2 Dropout Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dropout layer in DL helps reduce overfitting by introducing regularization and generalization capabilities into the model. In the literal sense, the dropout layer drops out a few neurons or sets them to 0 and reduces computation in the training process. The process of arbitrarily dropping neurons works quite well in reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7 Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The loss function is the metric that helps a network understand whether it is learning in the right direction.\n",
    "- Similarly, how does a network understand whether it is improving its learning process in each iteration? It uses the loss function. The loss function essentially measures the loss from the target. Say you are developing a model to predict whether a student will pass or fail and the chance of passing or failing is defined by the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example of Loss Functions for Regression Problem:\n",
    "    1. Mean Squared Error (MSE)\n",
    "    2. Mean Absolute Error (MAE)\n",
    "    3. Mean Absolute Percentage Error (MAPE)\n",
    "    4. Mean Squared Logarithmic Error (MSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example of Loss Functions for Classification Problem:\n",
    "    1. Binary Cross-Entropy\n",
    "    2. Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.8 Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most important part of the model training is the optimizer. Up to this point, we have addressed the process of giving feedback to the model through an algorithm called backpropagation; this is actually an optimization algorithm.\n",
    "- Optimizers is used to minimize Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example of Optimizers:\n",
    "    1. Stochastic Gradient Descent\n",
    "    2. Adam\n",
    "    3. RMSProp\n",
    "    4. Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each of the optimization techniques has its own pros and cons. A major problem which we often face in DL is the vanishing gradient and saddle point problem. You can explore these problems in more detail while choosing the best optimizer for your problem. But for most use cases, Adam always works fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.9 Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once you have designed your network, Keras provides you with an easy one-step model configuration process with the ‘compile’ command. To compile a model, we need to provide three parameters: an optimization function, a loss function, and a metric for the model to measure performance on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting seed for reproducibility\n",
    "np.random.seed(2018)\n",
    "\n",
    "#Generate Dummy Data\n",
    "x_train = np.random.random((6000, 10))\n",
    "y_train = np.random.randint(2, size=(6000,1))\n",
    "\n",
    "#Validation Data\n",
    "x_val = np.random.random((2000,10))\n",
    "y_val = np.random.randint(2, size=(2000, 1))\n",
    "\n",
    "#Testing Data\n",
    "x_test = np.random.random((2000,10))\n",
    "y_test = np.random.randint(2, size=(2000, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the model structure with the required layers, # of neurons, activation function and optimizers\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "#First hidden layer\n",
    "model.add(keras.layers.Dense(64, input_dim=10, activation='relu'))\n",
    "\n",
    "#Second hidden layer\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "#Third hidden layer\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "\n",
    "#Fourth hidden layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "#Fifth hidden layer\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "\n",
    "#Output layer\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Compile all layers\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model and Creating Predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\pythongpu2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 6000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "6000/6000 [==============================] - 22s 4ms/step - loss: 0.6934 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.5005\n",
      "Epoch 2/3\n",
      "6000/6000 [==============================] - 1s 172us/step - loss: 0.6934 - acc: 0.4967 - val_loss: 0.6928 - val_acc: 0.5025\n",
      "Epoch 3/3\n",
      "6000/6000 [==============================] - 1s 171us/step - loss: 0.6930 - acc: 0.5060 - val_loss: 0.6926 - val_acc: 0.5275\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3, validation_data=(x_val,y_val));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that after every epoch, the model prints the mean training loss and accuracy as well as the validation loss and accuracy. We can use these intermediate results to make a judgment on the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 95us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6932061853408813, 0.493]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice with Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the Dataset: <class 'numpy.ndarray'>\n",
      "Shape of training data : (404, 13)\n",
      "Shape of training labels : (404,)\n",
      "Shape of testing data : <class 'numpy.ndarray'>\n",
      "Shape of training data : (102, 13)\n",
      "Shape of testing labels : (102,)\n"
     ]
    }
   ],
   "source": [
    "#Explore the data structure using basic python commands\n",
    "\n",
    "print(\"Type of the Dataset:\",type(y_train))\n",
    "print(\"Shape of training data :\",x_train.shape)\n",
    "print(\"Shape of training labels :\",y_train.shape)\n",
    "#---------------------------------------\n",
    "print(\"Shape of testing data :\",type(x_test))\n",
    "print(\"Shape of training data :\",x_test.shape)\n",
    "print(\"Shape of testing labels :\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,\n",
       "        6.14200e+00, 9.17000e+01, 3.97690e+00, 4.00000e+00, 3.07000e+02,\n",
       "        2.10000e+01, 3.96900e+02, 1.87200e+01],\n",
       "       [2.17700e-02, 8.25000e+01, 2.03000e+00, 0.00000e+00, 4.15000e-01,\n",
       "        7.61000e+00, 1.57000e+01, 6.27000e+00, 2.00000e+00, 3.48000e+02,\n",
       "        1.47000e+01, 3.95380e+02, 3.11000e+00],\n",
       "       [4.89822e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.31000e-01,\n",
       "        4.97000e+00, 1.00000e+02, 1.33250e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.75520e+02, 3.26000e+00],\n",
       "       [3.96100e-02, 0.00000e+00, 5.19000e+00, 0.00000e+00, 5.15000e-01,\n",
       "        6.03700e+00, 3.45000e+01, 5.98530e+00, 5.00000e+00, 2.24000e+02,\n",
       "        2.02000e+01, 3.96900e+02, 8.01000e+00],\n",
       "       [3.69311e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.13000e-01,\n",
       "        6.37600e+00, 8.84000e+01, 2.56710e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.91430e+02, 1.46500e+01]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5 Top Rows Data\n",
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide 300 data for training and 104 data for validation\n",
    "x_train_2 = x_train[:300]\n",
    "y_train_2 = y_train[:300]\n",
    "x_val = x_train[300:]\n",
    "y_val = y_train[300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Architecture\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "#1 Hidden Layer\n",
    "model.add(keras.layers.Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "#2 Hidden Layer\n",
    "model.add(keras.layers.Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "#3 Hidden Layer\n",
    "model.add(keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#Compile Model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 404 samples, validate on 104 samples\n",
      "Epoch 1/3\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 573.3900 - mean_absolute_percentage_error: 98.1294 - val_loss: 650.2098 - val_mean_absolute_percentage_error: 96.3227\n",
      "Epoch 2/3\n",
      "404/404 [==============================] - 0s 294us/step - loss: 531.2772 - mean_absolute_percentage_error: 92.1745 - val_loss: 575.2983 - val_mean_absolute_percentage_error: 87.5916\n",
      "Epoch 3/3\n",
      "404/404 [==============================] - 0s 306us/step - loss: 431.8623 - mean_absolute_percentage_error: 77.5922 - val_loss: 419.7707 - val_mean_absolute_percentage_error: 67.2242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2cdf20780>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 158us/step\n",
      "loss  :  370.2984056659773\n",
      "mean_absolute_percentage_error  :  65.85465240478516\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "result = model.evaluate(x_test, y_test)\n",
    "for i in range(len(model.metrics_names)):\n",
    "    print(model.metrics_names[i],\" : \", result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that MAPE is around 65%, which is actually not a great number to have for model performance. This would translate into our model predictions at around 65% error. So, in general, if a house was priced at 10K, our model would have predicted ~17K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 404 samples, validate on 104 samples\n",
      "Epoch 1/30\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 586.8616 - mean_absolute_percentage_error: 100.2115 - val_loss: 679.4864 - val_mean_absolute_percentage_error: 99.8608\n",
      "Epoch 2/30\n",
      "404/404 [==============================] - 0s 371us/step - loss: 580.2191 - mean_absolute_percentage_error: 99.3068 - val_loss: 668.6140 - val_mean_absolute_percentage_error: 98.5669\n",
      "Epoch 3/30\n",
      "404/404 [==============================] - 0s 401us/step - loss: 563.7697 - mean_absolute_percentage_error: 96.9897 - val_loss: 637.1378 - val_mean_absolute_percentage_error: 95.0531\n",
      "Epoch 4/30\n",
      "404/404 [==============================] - 0s 383us/step - loss: 513.9567 - mean_absolute_percentage_error: 90.5087 - val_loss: 549.6228 - val_mean_absolute_percentage_error: 85.2074\n",
      "Epoch 5/30\n",
      "404/404 [==============================] - 0s 430us/step - loss: 405.0755 - mean_absolute_percentage_error: 74.7999 - val_loss: 387.4441 - val_mean_absolute_percentage_error: 63.8885\n",
      "Epoch 6/30\n",
      "404/404 [==============================] - 0s 369us/step - loss: 241.9307 - mean_absolute_percentage_error: 51.1514 - val_loss: 204.5635 - val_mean_absolute_percentage_error: 39.4432\n",
      "Epoch 7/30\n",
      "404/404 [==============================] - 0s 317us/step - loss: 133.3275 - mean_absolute_percentage_error: 41.6400 - val_loss: 130.2755 - val_mean_absolute_percentage_error: 39.3329\n",
      "Epoch 8/30\n",
      "404/404 [==============================] - 0s 290us/step - loss: 115.3843 - mean_absolute_percentage_error: 45.5519 - val_loss: 124.1249 - val_mean_absolute_percentage_error: 39.2258\n",
      "Epoch 9/30\n",
      "404/404 [==============================] - 0s 304us/step - loss: 107.0508 - mean_absolute_percentage_error: 41.5223 - val_loss: 121.9064 - val_mean_absolute_percentage_error: 34.7525\n",
      "Epoch 10/30\n",
      "404/404 [==============================] - 0s 263us/step - loss: 100.9315 - mean_absolute_percentage_error: 37.3247 - val_loss: 117.7229 - val_mean_absolute_percentage_error: 33.0438\n",
      "Epoch 11/30\n",
      "404/404 [==============================] - 0s 287us/step - loss: 94.8843 - mean_absolute_percentage_error: 36.4572 - val_loss: 110.1189 - val_mean_absolute_percentage_error: 33.1028\n",
      "Epoch 12/30\n",
      "404/404 [==============================] - 0s 273us/step - loss: 90.2190 - mean_absolute_percentage_error: 35.9680 - val_loss: 105.5713 - val_mean_absolute_percentage_error: 31.8140\n",
      "Epoch 13/30\n",
      "404/404 [==============================] - 0s 293us/step - loss: 85.7771 - mean_absolute_percentage_error: 34.5123 - val_loss: 101.6894 - val_mean_absolute_percentage_error: 30.5505\n",
      "Epoch 14/30\n",
      "404/404 [==============================] - 0s 290us/step - loss: 81.7966 - mean_absolute_percentage_error: 33.3339 - val_loss: 98.8441 - val_mean_absolute_percentage_error: 29.2253\n",
      "Epoch 15/30\n",
      "404/404 [==============================] - 0s 329us/step - loss: 78.4190 - mean_absolute_percentage_error: 32.2735 - val_loss: 95.2211 - val_mean_absolute_percentage_error: 28.6324\n",
      "Epoch 16/30\n",
      "404/404 [==============================] - 0s 286us/step - loss: 75.6576 - mean_absolute_percentage_error: 31.3111 - val_loss: 92.8191 - val_mean_absolute_percentage_error: 27.4798\n",
      "Epoch 17/30\n",
      "404/404 [==============================] - 0s 272us/step - loss: 73.0519 - mean_absolute_percentage_error: 31.1300 - val_loss: 89.5200 - val_mean_absolute_percentage_error: 27.4653\n",
      "Epoch 18/30\n",
      "404/404 [==============================] - 0s 278us/step - loss: 70.7478 - mean_absolute_percentage_error: 29.4324 - val_loss: 89.8545 - val_mean_absolute_percentage_error: 25.3748\n",
      "Epoch 19/30\n",
      "404/404 [==============================] - 0s 364us/step - loss: 68.2088 - mean_absolute_percentage_error: 29.3221 - val_loss: 86.1623 - val_mean_absolute_percentage_error: 25.8849\n",
      "Epoch 20/30\n",
      "404/404 [==============================] - 0s 288us/step - loss: 66.5359 - mean_absolute_percentage_error: 29.4220 - val_loss: 84.8616 - val_mean_absolute_percentage_error: 25.3011\n",
      "Epoch 21/30\n",
      "404/404 [==============================] - 0s 282us/step - loss: 65.0178 - mean_absolute_percentage_error: 28.2535 - val_loss: 86.0746 - val_mean_absolute_percentage_error: 24.0163\n",
      "Epoch 22/30\n",
      "404/404 [==============================] - 0s 297us/step - loss: 63.6382 - mean_absolute_percentage_error: 27.3348 - val_loss: 83.2157 - val_mean_absolute_percentage_error: 24.5687\n",
      "Epoch 23/30\n",
      "404/404 [==============================] - 0s 357us/step - loss: 62.7690 - mean_absolute_percentage_error: 27.8173 - val_loss: 82.8021 - val_mean_absolute_percentage_error: 24.2431\n",
      "Epoch 24/30\n",
      "404/404 [==============================] - 0s 332us/step - loss: 61.7121 - mean_absolute_percentage_error: 27.4133 - val_loss: 82.1960 - val_mean_absolute_percentage_error: 24.0173\n",
      "Epoch 25/30\n",
      "404/404 [==============================] - 0s 320us/step - loss: 60.6767 - mean_absolute_percentage_error: 27.0467 - val_loss: 81.9138 - val_mean_absolute_percentage_error: 23.7629\n",
      "Epoch 26/30\n",
      "404/404 [==============================] - 0s 362us/step - loss: 59.7791 - mean_absolute_percentage_error: 25.9791 - val_loss: 81.2244 - val_mean_absolute_percentage_error: 23.8424\n",
      "Epoch 27/30\n",
      "404/404 [==============================] - 0s 378us/step - loss: 59.3249 - mean_absolute_percentage_error: 26.2020 - val_loss: 80.4722 - val_mean_absolute_percentage_error: 23.9598\n",
      "Epoch 28/30\n",
      "404/404 [==============================] - 0s 345us/step - loss: 58.4887 - mean_absolute_percentage_error: 26.2096 - val_loss: 80.5790 - val_mean_absolute_percentage_error: 23.5989\n",
      "Epoch 29/30\n",
      "404/404 [==============================] - 0s 316us/step - loss: 58.2133 - mean_absolute_percentage_error: 25.9114 - val_loss: 80.9976 - val_mean_absolute_percentage_error: 23.2197\n",
      "Epoch 30/30\n",
      "404/404 [==============================] - 0s 361us/step - loss: 57.4215 - mean_absolute_percentage_error: 25.4456 - val_loss: 79.2000 - val_mean_absolute_percentage_error: 23.9286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2cede1f28>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets change epoch to 30\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 118us/step\n",
      "loss  :  62.62122988233379\n",
      "mean_absolute_percentage_error  :  30.606018216002223\n"
     ]
    }
   ],
   "source": [
    "#Evaluate again the model\n",
    "result = model.evaluate(x_test, y_test)\n",
    "for i in range(len(model.metrics_names)):\n",
    "    print(model.metrics_names[i],\" : \", result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MAPE has decreased almost half from 65% to 30%!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
